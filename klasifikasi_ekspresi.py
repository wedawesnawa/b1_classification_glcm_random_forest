# -*- coding: utf-8 -*-
"""Klasifikasi Ekspresi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hQXk-wEHznuPz_XIUSNF1OqigIhG1sOB

# Import Library
"""

# Impor library
import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
import seaborn as sns
import matplotlib.pyplot as plt
from skimage.feature import graycomatrix, graycoprops
import zipfile
import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from sklearn.ensemble import RandomForestClassifier

"""# Load Dataset"""

# Mengatur jalur dataset
dataset_path = "datasets(3s).zip"
dataset_dir = 'ckplus/datasets(3e)'

# Ekstrak file zip ke direktori 'ckplus'
with zipfile.ZipFile(dataset_path, 'r') as zip_ref:
    zip_ref.extractall('ckplus')

"""# Load Image dan Label"""

# Daftar class labels yang ingin diklasifikasikan
class_labels = ['angry', 'happy', 'sad']

# Load image paths and labels
def load_image_paths(dataset_dir):
    image_paths = []
    labels = []
    for root, _, files in os.walk(dataset_dir):
        # Extract label dari nama folder
        label = os.path.basename(root)
        # Pastikan label termasuk dalam class_labels
        if label in class_labels:
            for file in files:
                # Pastikan file merupakan citra dengan ekstensi yang diinginkan
                if file.endswith(".png") or file.endswith(".jpg"):
                    image_paths.append(os.path.join(root, file))
                    labels.append(label)
    return image_paths, labels

# Fungsi untuk menampilkan satu gambar acak dari setiap label
def display_one_random_image_per_label(image_paths, labels):
    fig, axes = plt.subplots(1, len(class_labels), figsize=(15, 3))

    for i, label in enumerate(class_labels):
        # Cari indeks gambar dengan label yang sesuai
        label_indices = [index for index, lbl in enumerate(labels) if lbl == label]
        random_index = random.choice(label_indices)
        img_path = image_paths[random_index]

        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        axes[i].imshow(img, cmap='gray')
        axes[i].set_title(label)
        axes[i].axis('off')

    plt.show()

# Memuat jalur gambar dan label dari dataset
image_paths, labels = load_image_paths(dataset_dir)

# Tampilkan satu gambar acak dari setiap label
display_one_random_image_per_label(image_paths, labels)

"""# Pre Processing + Ekstraksi Fitur (GLCM)"""

# Fungsi untuk mengekstraksi fitur GLCM dari gambar
def extract_greycomatrix_features(image, label):
    # Tentukan nilai angles dan distances berdasarkan label gambar
    if label == 'angry':
        angles = [0]
        distances = [1]
    elif label == 'happy':
        angles = [np.pi/2]
        distances = [5]
    elif label == 'sad':
        angles = [0]
        distances = [7]
    else:
        raise ValueError("Invalid label")

    # Hitung GLCM
    glcm = graycomatrix(image, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)

    # Ekstrak properti GLCM
    contrast = graycoprops(glcm, 'contrast').ravel()
    dissimilarity = graycoprops(glcm, 'dissimilarity').ravel()
    homogeneity = graycoprops(glcm, 'homogeneity').ravel()
    energy = graycoprops(glcm, 'energy').ravel()
    correlation = graycoprops(glcm, 'correlation').ravel()
    asm = graycoprops(glcm, 'ASM').ravel()

    # Gabungkan semua fitur menjadi satu vektor fitur
    features = np.hstack([contrast, dissimilarity, homogeneity, energy, correlation, asm])

    return features

# Fungsi untuk mengekstraksi fitur GLCM dari seluruh dataset
def extract_features_from_dataset(image_paths, labels):
    features_list = []
    for image_path, label in zip(image_paths, labels):
        # Baca gambar dalam skala abu-abu
        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

        # Resize gambar menjadi 48x48
        resized_img = cv2.resize(img, (48, 48))

        # Ekstraksi fitur GLCM
        glcm_features = extract_greycomatrix_features(resized_img, label)
        features_list.append(glcm_features)
    return np.array(features_list)

# Load image paths and labels
image_paths, labels = load_image_paths(dataset_dir)

# Mengekstraksi fitur GLCM dari dataset dengan pre-processing
glcm_features = extract_features_from_dataset(image_paths, labels)
print("Shape of GLCM features array:", glcm_features.shape)

# Fungsi untuk menampilkan hasil GLCM untuk beberapa gambar
def display_glcm_results(image_paths, labels, glcm_features, num_samples=3):
    # Generate random indices for selecting images
    random_indices = random.sample(range(len(image_paths)), num_samples)

    for idx in random_indices:
        # Select image path, label, and GLCM features
        image_path = image_paths[idx]
        label = labels[idx]
        features = glcm_features[idx]

        # Print label and GLCM features
        print(f"Label: {label}")
        print("GLCM features:")
        print(features)
        print()

        # Load original image
        original_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        resized_img = cv2.resize(original_img, (48, 48))  # Resize original image for preprocessing

        # Display original image
        plt.subplot(1, 2, 1)
        plt.imshow(original_img, cmap='gray')
        plt.title('Original Image')
        plt.axis('off')

        # Display preprocessed image
        plt.subplot(1, 2, 2)
        plt.imshow(resized_img, cmap='gray')
        plt.title('Preprocessed Image')
        plt.axis('off')

        plt.show()

# Contoh pemanggilan fungsi display_glcm_results
display_glcm_results(image_paths, labels, glcm_features, num_samples=3)

"""# Splitting Data"""

def visualize_train_test_split(labels, class_labels, train_labels, test_labels):
    # Encoding label
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(train_labels)
    y_test_encoded = label_encoder.transform(test_labels)

    # Hitung jumlah data training dan testing untuk setiap label
    train_label_counts = {label: y_train_encoded.tolist().count(label_encoder.transform([label])[0]) for label in class_labels}
    test_label_counts = {label: y_test_encoded.tolist().count(label_encoder.transform([label])[0]) for label in class_labels}

    # Visualisasikan jumlah data training dan testing untuk setiap label
    plt.figure(figsize=(10, 5))
    bar_width = 0.35
    index = np.arange(len(class_labels))

    # Plot bar chart
    bars1 = plt.bar(index, list(train_label_counts.values()), bar_width, label='Train')
    bars2 = plt.bar(index + bar_width, list(test_label_counts.values()), bar_width, label='Test')

    # Annotate the bars with counts
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width() / 2, height, '%d' % int(height), ha='center', va='bottom')

    plt.xlabel('Labels')
    plt.ylabel('Counts')
    plt.title('Train-Test Split for Each Label')
    plt.xticks(index + bar_width / 2, class_labels)
    plt.legend()

    plt.show()

# Split data menjadi training dan testing set
X_train, X_test, y_train, y_test = train_test_split(glcm_features, labels, test_size=0.2, random_state=42)

# Visualisasi pembagian data training dan testing untuk setiap label
visualize_train_test_split(labels, class_labels, y_train, y_test)

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split, KFold
import numpy as np
import pandas as pd

# Inisialisasi pipeline dengan Random Forest
pipeline_rf = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing dengan standard scaler
    ('rf', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42))  # Random Forest Classifier
])

# Inisialisasi LabelEncoder
label_encoder = LabelEncoder()

# Menggunakan LabelEncoder untuk mengubah label menjadi representasi numerik
y_train_encoded = label_encoder.fit_transform(y_train)

# Menggunakan k-fold cross-validation untuk prediksi
k_fold = KFold(n_splits=5, shuffle=True, random_state=42)

# List untuk menyimpan metrik evaluasi untuk setiap label dan setiap fold
accuracies = {label: [] for label in class_labels}
precision_scores = {label: [] for label in class_labels}
recall_scores = {label: [] for label in class_labels}
f1_scores = {label: [] for label in class_labels}

# List untuk menyimpan prediksi dan label sebenarnya dari setiap lipatan
all_y_true = []
all_y_pred = []

# Loop melalui setiap fold
for fold_idx, (train_idx, test_idx) in enumerate(k_fold.split(X_train), start=1):
    X_train_fold, X_test_fold = X_train[train_idx], X_train[test_idx]
    y_train_fold, y_test_fold = y_train_encoded[train_idx], y_train_encoded[test_idx]

    # Latih model pada fold saat ini
    pipeline_rf.fit(X_train_fold, y_train_fold)
    # Prediksi label untuk data uji di fold saat ini
    y_pred_fold = pipeline_rf.predict(X_test_fold)

    # Hitung metrik evaluasi untuk setiap label dan simpan dalam list
    for label in class_labels:
        idx = label_encoder.transform([label])[0]
        label_y_test_fold = (y_test_fold == idx)
        label_y_pred_fold = (y_pred_fold == idx)
        if any(label_y_test_fold):  # Cek apakah ada prediksi yang benar untuk label ini
            accuracy = accuracy_score(label_y_test_fold, label_y_pred_fold)
            precision = precision_score(label_y_test_fold, label_y_pred_fold, zero_division=0)
            recall = recall_score(label_y_test_fold, label_y_pred_fold, zero_division=0)
            f1 = f1_score(label_y_test_fold, label_y_pred_fold, zero_division=0)
        else:
        accuracy = precision = recall = f1 = 0  # Set nilai default jika tidak ada prediksi yang benar
        accuracies[label].append(accuracy)
        precision_scores[label].append(precision)
        recall_scores[label].append(recall)
        f1_scores[label].append(f1)

    # Simpan prediksi dan label sebenarnya dari fold saat ini
    all_y_true.extend(y_test_fold)
    all_y_pred.extend(y_pred_fold)

# Buat DataFrame kosong untuk menyimpan hasil k-fold cross-validation
results_df = pd.DataFrame(index=range(1, k_fold.n_splits + 1), columns=[f'{label}_{metric}' for label in class_labels for metric in ['accuracy', 'precision', 'recall', 'f1']])

# Isi DataFrame dengan hasil metrik evaluasi untuk setiap label dan setiap fold
for label in class_labels:
    results_df[f'{label}_accuracy'] = accuracies[label]
    results_df[f'{label}_precision'] = precision_scores[label]
    results_df[f'{label}_recall'] = recall_scores[label]
    results_df[f'{label}_f1'] = f1_scores[label]

# Tambahkan baris terakhir untuk menampilkan rata-rata dari setiap kolom
results_df.loc['average'] = results_df.mean()

# Tampilkan DataFrame
print("Hasil Metrics K-Fold Cross Validation:")
print(results_df)

# Hitung laporan klasifikasi berdasarkan gabungan prediksi dan label sebenarnya
classification_report_kfold = classification_report(all_y_true, all_y_pred, target_names=class_labels)

# Tampilkan laporan klasifikasi berdasarkan k-fold cross-validation
print("\nClassification Report (K-Fold Cross Validation):")
print(classification_report_kfold)

# Data testing
# Prediksi label untuk data testing (20%)
y_test_encoded = label_encoder.transform(y_test)
y_pred_test = pipeline_rf.predict(X_test)

# Hitung laporan klasifikasi berdasarkan data testing
classification_report_test = classification_report(y_test_encoded, y_pred_test, target_names=class_labels)

# Tampilkan laporan klasifikasi berdasarkan data testing
print("\nClassification Report (Test Data):")
print(classification_report_test)

# Inisialisasi pipeline dengan Random Forest
pipeline_rf = Pipeline([
    ('scaler', StandardScaler()),  # Preprocessing dengan standard scaler
    ('rf', RandomForestClassifier(random_state=42))  # Random Forest Classifier
])

# Inisialisasi LabelEncoder
label_encoder = LabelEncoder()

# Menggunakan LabelEncoder untuk mengubah label menjadi representasi numerik
y_train_encoded = label_encoder.fit_transform(y_train)

# Parameter grid untuk Random Forest
param_grid = {
    'rf__n_estimators': [100, 150, 200],
    'rf__max_depth': [None, 10, 20, 30],
}

# Inisialisasi GridSearchCV dengan 5-fold cross-validation
grid_search = GridSearchCV(pipeline_rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Latih GridSearchCV pada data pelatihan
grid_search.fit(X_train, y_train_encoded)

# Menampilkan parameter terbaik
print("Best parameters found:", grid_search.best_params_)